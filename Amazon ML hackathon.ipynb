{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-01T09:35:07.246317Z","iopub.execute_input":"2021-08-01T09:35:07.247121Z","iopub.status.idle":"2021-08-01T09:35:07.267489Z","shell.execute_reply.started":"2021-08-01T09:35:07.246984Z","shell.execute_reply":"2021-08-01T09:35:07.266373Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/amazon-ml-challenge-2021-hackerearth/sample_submission.csv\n/kaggle/input/amazon-ml-challenge-2021-hackerearth/train.csv\n/kaggle/input/amazon-ml-challenge-2021-hackerearth/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport re\nimport nltk\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2021-08-01T09:35:07.569777Z","iopub.execute_input":"2021-08-01T09:35:07.570257Z","iopub.status.idle":"2021-08-01T09:35:09.165832Z","shell.execute_reply.started":"2021-08-01T09:35:07.570215Z","shell.execute_reply":"2021-08-01T09:35:09.164903Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB","metadata":{"execution":{"iopub.status.busy":"2021-08-01T09:35:09.167100Z","iopub.execute_input":"2021-08-01T09:35:09.167433Z","iopub.status.idle":"2021-08-01T09:35:09.175780Z","shell.execute_reply.started":"2021-08-01T09:35:09.167402Z","shell.execute_reply":"2021-08-01T09:35:09.174850Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/amazon-ml-challenge-2021-hackerearth/train.csv',escapechar=\"\\\\\",quoting=3)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-01T09:35:09.177372Z","iopub.execute_input":"2021-08-01T09:35:09.177699Z","iopub.status.idle":"2021-08-01T09:36:18.183201Z","shell.execute_reply.started":"2021-08-01T09:35:09.177670Z","shell.execute_reply":"2021-08-01T09:36:18.182109Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                               TITLE  \\\n0         Pete The Cat Bedtime Blues Doll, 14.5 Inch   \n1  The New Yorker NYHM014 Refrigerator Magnet, 2 ...   \n2  The Ultimate Self-Sufficiency Handbook: A Comp...   \n3   Amway Nutrilite Kids Chewable Iron Tablets (100)   \n4  Teacher Planner Company A4 6 Lesson Academic T...   \n\n                                         DESCRIPTION  \\\n0  Pete the Cat is the coolest, most popular cat ...   \n1  The New Yorker Handsome Cello Wrapped Hard Mag...   \n2                                                NaN   \n3                                                NaN   \n4                                                NaN   \n\n                                       BULLET_POINTS           BRAND  \\\n0  [Pete the Cat Bedtime Blues plush doll,Based o...     MerryMakers   \n1  [Cat In A Tea Cup by New Yorker cover artist G...  The New Yorker   \n2                                Skyhorse Publishing          imusti   \n3  [Nutrilite Kids,Chewable Iron Tablets,Quantity...           Amway   \n4                                                NaN             NaN   \n\n   BROWSE_NODE_ID  \n0               0  \n1               1  \n2               2  \n3               3  \n4               4  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TITLE</th>\n      <th>DESCRIPTION</th>\n      <th>BULLET_POINTS</th>\n      <th>BRAND</th>\n      <th>BROWSE_NODE_ID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Pete The Cat Bedtime Blues Doll, 14.5 Inch</td>\n      <td>Pete the Cat is the coolest, most popular cat ...</td>\n      <td>[Pete the Cat Bedtime Blues plush doll,Based o...</td>\n      <td>MerryMakers</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>The New Yorker NYHM014 Refrigerator Magnet, 2 ...</td>\n      <td>The New Yorker Handsome Cello Wrapped Hard Mag...</td>\n      <td>[Cat In A Tea Cup by New Yorker cover artist G...</td>\n      <td>The New Yorker</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The Ultimate Self-Sufficiency Handbook: A Comp...</td>\n      <td>NaN</td>\n      <td>Skyhorse Publishing</td>\n      <td>imusti</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Amway Nutrilite Kids Chewable Iron Tablets (100)</td>\n      <td>NaN</td>\n      <td>[Nutrilite Kids,Chewable Iron Tablets,Quantity...</td>\n      <td>Amway</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Teacher Planner Company A4 6 Lesson Academic T...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df = df.mask(pd.isnull,df['TITLE'],axis=0)\ndf = df.mask(pd.isnull,df['DESCRIPTION'],axis=0)\ndf = df.mask(pd.isnull,df['BULLET_POINTS'],axis=0)\ndf = df.dropna()\n\nprint(np.sum(df.isnull())/len(df))\nprint(df.head())","metadata":{"execution":{"iopub.status.busy":"2021-08-01T09:36:18.185018Z","iopub.execute_input":"2021-08-01T09:36:18.185333Z","iopub.status.idle":"2021-08-01T09:36:29.604911Z","shell.execute_reply.started":"2021-08-01T09:36:18.185303Z","shell.execute_reply":"2021-08-01T09:36:29.603836Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"TITLE             0.0\nDESCRIPTION       0.0\nBULLET_POINTS     0.0\nBRAND             0.0\nBROWSE_NODE_ID    0.0\ndtype: float64\n                                               TITLE  \\\n0         Pete The Cat Bedtime Blues Doll, 14.5 Inch   \n1  The New Yorker NYHM014 Refrigerator Magnet, 2 ...   \n2  The Ultimate Self-Sufficiency Handbook: A Comp...   \n3   Amway Nutrilite Kids Chewable Iron Tablets (100)   \n4  Teacher Planner Company A4 6 Lesson Academic T...   \n\n                                         DESCRIPTION  \\\n0  Pete the Cat is the coolest, most popular cat ...   \n1  The New Yorker Handsome Cello Wrapped Hard Mag...   \n2  The Ultimate Self-Sufficiency Handbook: A Comp...   \n3   Amway Nutrilite Kids Chewable Iron Tablets (100)   \n4  Teacher Planner Company A4 6 Lesson Academic T...   \n\n                                       BULLET_POINTS  \\\n0  [Pete the Cat Bedtime Blues plush doll,Based o...   \n1  [Cat In A Tea Cup by New Yorker cover artist G...   \n2                                Skyhorse Publishing   \n3  [Nutrilite Kids,Chewable Iron Tablets,Quantity...   \n4  Teacher Planner Company A4 6 Lesson Academic T...   \n\n                                               BRAND  BROWSE_NODE_ID  \n0                                        MerryMakers               0  \n1                                     The New Yorker               1  \n2                                             imusti               2  \n3                                              Amway               3  \n4  Teacher Planner Company A4 6 Lesson Academic T...               4  \n","output_type":"stream"}]},{"cell_type":"code","source":"from nltk.corpus import stopwords\nstop_words_set = set(stopwords.words('english'))\n\ncount_list = df['BROWSE_NODE_ID'].value_counts().head(3000)\nprint(count_list)\nprint(\"\\n ##################################################### \\n\")\nprint(f\"Number of data points : {np.sum(count_list)}\")\ndf.head()\n\n################## Multinomial Naive bayes ############################\nclf = MultinomialNB()\ncount_vect = CountVectorizer(max_features=2000,stop_words=stop_words_set)\n\ntokens_title         = set()\ntokens_decription    = set()\ntokens_bullet_points = set()\n\nfor j in range(30):\n    train_rows = []\n    for i in range(len(count_list)):\n        num_rows = round(count_list.iloc[i]/50)\n        #print(\"######################################################\\n\")\n        #print(f\" Number of elements selected from class {df.index.values[i]} : {num_rows}\\n\")\n        row_list = np.random.choice(df[df['BROWSE_NODE_ID']==count_list.index.values[i]].index.values,num_rows)\n        #print(row_list[:5])\n        train_rows.extend(row_list)\n        print(f\"############## {i+1} ################\",end='\\r')\n    print(\"\\n\")\n    #print(len(train_rows))\n\n    # Shuffle row_list\n    np.random.shuffle(train_rows)\n\n    ############## Train data  #######################\n    df_train  = df.loc[train_rows]\n    \n    ############## Epoch i training ##################\n    X_train =  df_train[['TITLE','DESCRIPTION','BULLET_POINTS']] \n    #y_train = df_train['BROWSE_NODE_ID'], random_state = 0,test_size=0.0)\n    X_train_counts_1 = count_vect.fit_transform(X_train['TITLE']).toarray()\n    tokens_title.update(set(count_vect.get_feature_names()))\n    \n    X_train_counts_2 = count_vect.fit_transform(X_train['DESCRIPTION']).toarray()\n    tokens_decription.update(set(count_vect.get_feature_names()))\n    \n    X_train_counts_3 = count_vect.fit_transform(X_train['BULLET_POINTS']).toarray()\n    tokens_bullet_points.update(set(count_vect.get_feature_names()))\n    \n    print(f\"Title: {len(tokens_title)} :: Description: {len(tokens_decription)} :: Bullet Points: {len(tokens_bullet_points)}\")\n    \n    print(f\"############################  {j} ###################################\")\n    \n    #X_train_counts = np.hstack((X_train_counts_1,X_train_counts_2,X_train_counts_3))\n    #print(X_train_counts.shape)\n    \n    #tfidf_transformer = TfidfTransformer()\n    #X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts).toarray()\n    #print(X_train_tfidf.shape)\n\n    #clf.fit(X_train_tfidf, y_train)\n    \n    # Prediction\n    #X_test_counts_1 = count_vect.transform(X_test['TITLE']).toarray()\n    #X_test_counts_2 = count_vect.transform(X_test['DESCRIPTION']).toarray()\n    #X_test_counts_3 = count_vect.transform(X_test['BULLET_POINTS']).toarray()\n    #X_test_counts = np.hstack((X_test_counts_1,X_test_counts_2,X_test_counts_3))\n    #print(X_test_counts.shape)\n    \n    #y_pred = clf.predict(X_test_counts)\n    #accuracy = np.sum(y_pred==y_test)/len(y_test)\n    #print(f\"###########Epoch {j} : {accuracy} ##########\")","metadata":{"execution":{"iopub.status.busy":"2021-08-01T10:48:49.747646Z","iopub.execute_input":"2021-08-01T10:48:49.748024Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"1045      215698\n5          70318\n1251       51929\n1052       45553\n4          34169\n           ...  \n110169        73\n3902          73\n25122         72\n34978         72\n16263         72\nName: BROWSE_NODE_ID, Length: 3000, dtype: int64\n\n ##################################################### \n\nNumber of data points : 2778244\n############## 3000 ################\n\nTitle: 2000 :: Description: 2000 :: Bullet Points: 2000\n############################  0 ###################################\n############## 3000 ################\n\nTitle: 2138 :: Description: 2081 :: Bullet Points: 2075\n############################  1 ###################################\n############## 3000 ################\n\nTitle: 2218 :: Description: 2116 :: Bullet Points: 2112\n############################  2 ###################################\n############## 3000 ################\n\nTitle: 2265 :: Description: 2147 :: Bullet Points: 2134\n############################  3 ###################################\n############## 3000 ################\n\nTitle: 2298 :: Description: 2164 :: Bullet Points: 2152\n############################  4 ###################################\n############## 3000 ################\n\n","output_type":"stream"}]},{"cell_type":"code","source":"print(len(tokens_title))\nprint(len(tokens_decription))\nprint(len(tokens_bullet_points))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count_list = df['BROWSE_NODE_ID'].value_counts().count(3000)\nprint(count_list)\nprint(\"\\n ##################################################### \\n\")\nprint(f\"Number of data points : {len(count_list)}\")\ndf.head()\nclasses_list = list(count_list.index)\n\n################## Multinomial Naive bayes ############################\nclf = MultinomialNB()\nprint(len(tokens_title))\nprint(len(tokens_decription))\nprint(len(tokens_bullet_points))\ncount_vect_title         = CountVectorizer(vocabulary=tokens_title)\ncount_vect_decription    = CountVectorizer(vocabulary=tokens_decription)\ncount_vect_bullet_points = CountVectorizer(vocabulary=tokens_bullet_points)\n\nfor j in range(50):\n    train_rows = []\n    for i in range(len(count_list)):\n        num_rows = round(count_list.iloc[i]/100)\n        if(num_rows<1):\n            num_rows = 1\n        #print(\"######################################################\\n\")\n        #print(f\" Number of elements selected from class {df.index.values[i]} : {num_rows}\\n\")\n        row_list = np.random.choice(df[df['BROWSE_NODE_ID']==count_list.index.values[i]].index.values,num_rows)\n        #print(row_list[:5])\n        train_rows.extend(row_list)\n        print(f\"############## {i+1} ################\",end='\\r')\n    print(\"\\n\")\n    #print(len(train_rows))\n\n    # Shuffle row_list\n    np.random.shuffle(train_rows)\n\n    ############## Train data  #######################\n    df_train  = df.loc[train_rows]\n    \n    ############## Epoch i training ##################\n    X_train, X_test, y_train, y_test = train_test_split(df_train[['TITLE','DESCRIPTION','BULLET_POINTS']], df_train['BROWSE_NODE_ID'], random_state = 0)\n    X_train_counts_1 = count_vect_title.fit_transform(X_train['TITLE']).toarray()\n    X_train_counts_2 = count_vect_decription.fit_transform(X_train['DESCRIPTION']).toarray()\n    X_train_counts_3 = count_vect_bullet_points.fit_transform(X_train['BULLET_POINTS']).toarray()\n    X_train_counts = np.hstack((X_train_counts_1,X_train_counts_2,X_train_counts_3))\n    print([X_train_counts.shape,X_train_counts_1.shape,X_train_counts_2.shape,X_train_counts_3.shape])\n    \n    tfidf_transformer = TfidfTransformer()\n    X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts).toarray()\n    print(X_train_tfidf.shape)\n\n    clf.partial_fit(X_train_tfidf, y_train,classes=classes_list)\n    \n    # Prediction\n    X_test_counts_1 = count_vect_title.transform(X_test['TITLE']).toarray()\n    X_test_counts_2 = count_vect_decription.transform(X_test['DESCRIPTION']).toarray()\n    X_test_counts_3 = count_vect_bullet_points.transform(X_test['BULLET_POINTS']).toarray()\n    X_test_counts = np.hstack((X_test_counts_1,X_test_counts_2,X_test_counts_3))\n    print(X_test_counts.shape)\n    \n    y_pred = clf.predict(X_test_counts)\n    accuracy = np.sum(y_pred==y_test)/len(y_test)\n    print(f\"###########Epoch {j} : {accuracy} ##########\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\nsgd_clf = SGDClassifier(n_jobs=-1)\n\ncount_list = df['BROWSE_NODE_ID'].value_counts().head(1000)\nprint(count_list)\nprint(\"\\n ##################################################### \\n\")\nprint(f\"Number of data points : {len(df)}\")\ndf.head()\n\n################## Multinomial Naive bayes ############################\ncount_vect_title         = CountVectorizer(vocabulary=tokens_title)\ncount_vect_decription    = CountVectorizer(vocabulary=tokens_decription)\ncount_vect_bullet_points = CountVectorizer(vocabulary=tokens_bullet_points)\n\nfor j in range(50):\n    train_rows = []\n    for i in range(len(count_list)):\n        num_rows = round(count_list.iloc[i]/100)\n        #print(\"######################################################\\n\")\n        #print(f\" Number of elements selected from class {df.index.values[i]} : {num_rows}\\n\")\n        row_list = np.random.choice(df[df['BROWSE_NODE_ID']==count_list.index.values[i]].index.values,num_rows)\n        #print(row_list[:5])\n        train_rows.extend(row_list)\n        print(f\"############## {i+1} ################\",end='\\r')\n    print(\"\\n\")\n    #print(len(train_rows))\n\n    # Shuffle row_list\n    np.random.shuffle(train_rows)\n\n    ############## Train data  #######################\n    df_train  = df.loc[train_rows]\n    \n    ############## Epoch i training ##################\n    X_train, X_test, y_train, y_test = train_test_split(df_train[['TITLE','DESCRIPTION','BULLET_POINTS']],\n                                                        df_train['BROWSE_NODE_ID'], random_state = 0,test_size=0.2)\n    X_train_counts_1 = count_vect_title.fit_transform(X_train['TITLE']).toarray()\n    X_train_counts_2 = count_vect_decription.fit_transform(X_train['DESCRIPTION']).toarray()\n    X_train_counts_3 = count_vect_bullet_points.fit_transform(X_train['BULLET_POINTS']).toarray()\n    X_train_counts = np.hstack((X_train_counts_1,X_train_counts_2,X_train_counts_3))\n    print(X_train_counts.shape)\n    \n    tfidf_transformer = TfidfTransformer()\n    X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts).toarray()\n    print(X_train_tfidf.shape)\n\n    sgd_clf.partial_fit(X_train_tfidf, y_train,classes=classes_list)\n    \n    # Prediction\n    X_test_counts_1 = count_vect_title.transform(X_test['TITLE']).toarray()\n    X_test_counts_2 = count_vect_decription.transform(X_test['DESCRIPTION']).toarray()\n    X_test_counts_3 = count_vect_bullet_points.transform(X_test['BULLET_POINTS']).toarray()\n    X_test_counts = np.hstack((X_test_counts_1,X_test_counts_2,X_test_counts_3))\n    print(X_test_counts.shape)\n    \n    y_pred = sgd_clf.predict(X_test_counts)\n    accuracy = np.sum(y_pred==y_test)/len(y_test)\n    print(f\"###########Epoch {j} : {accuracy} ##########\")","metadata":{"execution":{"iopub.status.busy":"2021-08-01T10:20:43.447304Z","iopub.execute_input":"2021-08-01T10:20:43.447883Z","iopub.status.idle":"2021-08-01T10:25:29.744473Z","shell.execute_reply.started":"2021-08-01T10:20:43.447847Z","shell.execute_reply":"2021-08-01T10:25:29.742940Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"1045     215698\n5         70318\n1251      51929\n1052      45553\n4         34169\n          ...  \n1344        417\n8939        416\n615         415\n10495       414\n6980        414\nName: BROWSE_NODE_ID, Length: 1000, dtype: int64\n\n ##################################################### \n\nNumber of data points : 2903014\n############## 1000 ################\n\n(19349, 3470)\n(19349, 3470)\n(4838, 3470)\n###########Epoch 0 : 0.5183960314179413 ##########\n############## 1000 ################\n\n(19349, 3470)\n(19349, 3470)\n(4838, 3470)\n###########Epoch 1 : 0.5324514262091773 ##########\n############## 1000 ################\n\n(19349, 3470)\n(19349, 3470)\n(4838, 3470)\n###########Epoch 2 : 0.5363786688714345 ##########\n############## 1000 ################\n\n(19349, 3470)\n(19349, 3470)\n(4838, 3470)\n###########Epoch 3 : 0.5367920628358825 ##########\n############## 1000 ################\n\n(19349, 3470)\n(19349, 3470)\n(4838, 3470)\n###########Epoch 4 : 0.5239768499379909 ##########\n############## 1000 ################\n\n(19349, 3470)\n(19349, 3470)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-3ef49b655dce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# Prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mX_test_counts_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vect_title\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TITLE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mX_test_counts_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vect_decription\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DESCRIPTION'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0mX_test_counts_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vect_bullet_points\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'BULLET_POINTS'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mX_test_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_counts_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test_counts_2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test_counts_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1040\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m         \u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m         \u001b[0mcsr_todense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"#### Test and evaluation\n\ntest_df = pd.read_csv('../input/amazon-ml-challenge-2021-hackerearth/test.csv',escapechar=\"\\\\\",quoting=3)\ntest_df = test_df.mask(pd.isnull,test_df['TITLE'],axis=0)\ntest_df = test_df.mask(pd.isnull,test_df['DESCRIPTION'],axis=0)\ntest_df = test_df.mask(pd.isnull,test_df['BULLET_POINTS'],axis=0)\ntest_df = test_df.dropna()\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-01T10:10:01.956449Z","iopub.execute_input":"2021-08-01T10:10:01.956928Z","iopub.status.idle":"2021-08-01T10:10:04.138844Z","shell.execute_reply.started":"2021-08-01T10:10:01.956883Z","shell.execute_reply":"2021-08-01T10:10:04.137727Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"   PRODUCT_ID                                              TITLE  \\\n0           1  Command 3M Small Kitchen Hooks, White, Decorat...   \n1           2  O'Neal Jump Hardware JAG Unisex-Adult Glove (B...   \n2           3  NFL Detroit Lions Portable Party Fridge, 15.8 ...   \n3           4  Panasonic Single Line KX-TS880MX Corded Phone ...   \n4           5  Zero Baby Girl's 100% Cotton Innerwear Bloomer...   \n\n                                         DESCRIPTION  \\\n0                                    Sale Unit: PACK   \n1  Synthetic leather palm with double-layer thumb...   \n2  Boelter Brands lets you celebrate your favorit...   \n3  Features: 50 Station Phonebook Corded Phone Al...   \n4  Zero Baby Girl Panties Set. 100% Cotton, Breat...   \n\n                                       BULLET_POINTS           BRAND  \n0  [INCLUDES - 9 hooks and 12 small indoor strips...         Command  \n1  [Silicone printing for a better grip. Long las...          O'Neal  \n2  [Runs on 12 Volt DC Power or 110 Volt AC Power...  Boelter Brands  \n3  Panasonic Landline Phones doesn't come with a ...       Panasonic  \n4  [Zero Baby Girl Panties, Pack of 6, 100% Cotto...            Zero  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PRODUCT_ID</th>\n      <th>TITLE</th>\n      <th>DESCRIPTION</th>\n      <th>BULLET_POINTS</th>\n      <th>BRAND</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Command 3M Small Kitchen Hooks, White, Decorat...</td>\n      <td>Sale Unit: PACK</td>\n      <td>[INCLUDES - 9 hooks and 12 small indoor strips...</td>\n      <td>Command</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>O'Neal Jump Hardware JAG Unisex-Adult Glove (B...</td>\n      <td>Synthetic leather palm with double-layer thumb...</td>\n      <td>[Silicone printing for a better grip. Long las...</td>\n      <td>O'Neal</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>NFL Detroit Lions Portable Party Fridge, 15.8 ...</td>\n      <td>Boelter Brands lets you celebrate your favorit...</td>\n      <td>[Runs on 12 Volt DC Power or 110 Volt AC Power...</td>\n      <td>Boelter Brands</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>Panasonic Single Line KX-TS880MX Corded Phone ...</td>\n      <td>Features: 50 Station Phonebook Corded Phone Al...</td>\n      <td>Panasonic Landline Phones doesn't come with a ...</td>\n      <td>Panasonic</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>Zero Baby Girl's 100% Cotton Innerwear Bloomer...</td>\n      <td>Zero Baby Girl Panties Set. 100% Cotton, Breat...</td>\n      <td>[Zero Baby Girl Panties, Pack of 6, 100% Cotto...</td>\n      <td>Zero</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test_df = test_df.dropna()\n\ncount_vect_title         = CountVectorizer(vocabulary=tokens_title)\ncount_vect_decription    = CountVectorizer(vocabulary=tokens_decription)\ncount_vect_bullet_points = CountVectorizer(vocabulary=tokens_bullet_points)\n\nX_test_1 = test_df['TITLE']\nX_test_2 = test_df['DESCRIPTION']\nX_test_3 = test_df['BULLET_POINTS']\n\nX_test_counts_1 = count_vect_title.fit_transform(X_test_1).toarray()\nX_test_counts_2 = count_vect_decription.fit_transform(X_test_2).toarray()\nX_test_counts_3 = count_vect_bullet_points.fit_transform(X_test_3).toarray()\nX_test_counts   = np.hstack((X_test_counts_1,X_test_counts_2,X_test_counts_3))\nprint(X_test_counts.shape)\ny_pred = clf.predict(X_test_counts)","metadata":{"execution":{"iopub.status.busy":"2021-08-01T10:10:16.836065Z","iopub.execute_input":"2021-08-01T10:10:16.836439Z","iopub.status.idle":"2021-08-01T10:10:55.651720Z","shell.execute_reply.started":"2021-08-01T10:10:16.836407Z","shell.execute_reply":"2021-08-01T10:10:55.650970Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"(110775, 3470)\n","output_type":"stream"}]},{"cell_type":"code","source":"test_df = test_df.drop(['TITLE','DESCRIPTION','BULLET_POINTS','BRAND'],axis=1)\ntest_df['BROWSE_NODE_ID'] = y_pred\ntest_df = test_df.set_index('PRODUCT_ID')\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-01T10:10:55.653081Z","iopub.execute_input":"2021-08-01T10:10:55.653507Z","iopub.status.idle":"2021-08-01T10:10:55.665604Z","shell.execute_reply.started":"2021-08-01T10:10:55.653477Z","shell.execute_reply":"2021-08-01T10:10:55.664879Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"            BROWSE_NODE_ID\nPRODUCT_ID                \n1                       74\n2                     2321\n3                       81\n4                      604\n5                     1239","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>BROWSE_NODE_ID</th>\n    </tr>\n    <tr>\n      <th>PRODUCT_ID</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>74</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2321</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>81</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>604</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1239</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test_df.to_csv('./result_nb.csv')","metadata":{"execution":{"iopub.status.busy":"2021-08-01T10:10:58.761141Z","iopub.execute_input":"2021-08-01T10:10:58.761737Z","iopub.status.idle":"2021-08-01T10:10:59.030625Z","shell.execute_reply.started":"2021-08-01T10:10:58.761693Z","shell.execute_reply":"2021-08-01T10:10:59.029807Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_1 = clf","metadata":{"execution":{"iopub.status.busy":"2021-08-01T10:40:50.069006Z","iopub.status.idle":"2021-08-01T10:40:50.069504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}